{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:00:52.349102Z",
     "start_time": "2020-10-06T08:00:52.256810Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('..')\n",
    "from twitterED.utilities.utils import text_cleaning\n",
    "# import string\n",
    "# import unicodedata\n",
    "# import re\n",
    "from wordcloud import WordCloud\n",
    "# import seaborn as sns\n",
    "# from pylab import rcParams\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "# from matplotlib import rc\n",
    "# from tqdm.notebook import tqdm,trange\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:55:14.284090Z",
     "start_time": "2020-10-06T07:55:14.252909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Would you like to interest hundreds of premium...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antonio comes across as the most down to earth...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I learned a lot about COVID. This is the schoo...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does Laurence Fox even have anything worth...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>still thinking about that morph cut they used ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  emotion\n",
       "0  Would you like to interest hundreds of premium...     fear\n",
       "1  Antonio comes across as the most down to earth...     fear\n",
       "2  I learned a lot about COVID. This is the schoo...     fear\n",
       "3      Does Laurence Fox even have anything worth...  sadness\n",
       "4  still thinking about that morph cut they used ...     fear"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/processed/emotions/ED.csv',header=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:55:15.836946Z",
     "start_time": "2020-10-06T07:55:15.372385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting count account type\n",
    "ax =dataset['emotion'].value_counts().plot.bar(x='lab', y='val', rot=0,color = list('rgbkymc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:58:27.704827Z",
     "start_time": "2020-10-06T07:58:27.587868Z"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning text\n",
    "# fixing contarctions\n",
    "dataset['Tweet']= dataset['Tweet'].apply(text_cleaning.fix_contractions)\n",
    "# remove accent characters\n",
    "dataset['Tweet']= dataset['Tweet'].apply(text_cleaning.remove_accented_chars)\n",
    "# remove digits\n",
    "dataset['Tweet']= dataset['Tweet'].apply(text_cleaning.remove_digits)\n",
    "#remove excess withe spaces\n",
    "dataset['Tweet']= dataset['Tweet'].apply(text_cleaning.remove_excess_whitespace)\n",
    "# remove punctuations\n",
    "dataset['Tweet']= dataset['Tweet'].apply(text_cleaning.rm_punctuation1)\n",
    "# teurn to lower case\n",
    "dataset['Tweet']= dataset['Tweet'].apply(text_cleaning.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emotion separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:10:56.953485Z",
     "start_time": "2020-10-06T08:10:56.945484Z"
    }
   },
   "outputs": [],
   "source": [
    "# slicing dataset based on given emotion\n",
    "\n",
    "def emo_df(emotion,dataset,column='emotion'):\n",
    "    return dataset[dataset[column] == emotion].reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:12:28.053167Z",
     "start_time": "2020-10-06T08:12:28.041170Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = emo_df('fear',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:12:38.747657Z",
     "start_time": "2020-10-06T08:12:38.729664Z"
    }
   },
   "outputs": [],
   "source": [
    "# # selecting target emotion\n",
    "# dataset = sadness.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:13:47.267325Z",
     "start_time": "2020-10-06T08:13:46.616459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'you', 'like', 'to', 'interest', 'hundreds', 'of', 'premium', 'quality', 'qualified', 'prospects', 'to', 'consider', 'ones', 'business', 'every', 'day', 'in', 'this', 'case', 'social', 'media', 'is', 'perfect', 'facts', 'can', 'be', 'found', 'here', 'https']\n"
     ]
    }
   ],
   "source": [
    "# tokenising\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence)))  # deacc=True removes punctuations\n",
    "\n",
    "data = dataset.Tweet.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:14:08.222363Z",
     "start_time": "2020-10-06T08:14:07.079609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:24.038270Z",
     "start_time": "2020-10-05T12:01:24.023236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.extend(['would', 'from', 'say','get','take','use','go','https'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:24.209984Z",
     "start_time": "2020-10-05T12:01:24.200984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.608828Z",
     "start_time": "2020-10-05T12:01:24.376540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'worth', 'take', 'libel', 'case', 'mean', 'hea', 'pretty', 'much', 'unemployed', 'unemployable', 'didnat', 'leave', 'much', 'dump', 'ass']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.623848Z",
     "start_time": "2020-10-05T12:01:25.610827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "import pickle\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "#saving dictionary\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "id2word.save('dictionary.gensim')\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.639826Z",
     "start_time": "2020-10-05T12:01:25.625826Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Build LDA model\n",
    "# lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "#                                        id2word=id2word,\n",
    "#                                        num_topics=10, \n",
    "#                                        random_state=100,\n",
    "#                                        chunksize=100,\n",
    "#                                        passes=10,\n",
    "#                                        per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.654824Z",
     "start_time": "2020-10-05T12:01:25.641828Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# # Print the Keyword in the 10 topics\n",
    "# pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.670007Z",
     "start_time": "2020-10-05T12:01:25.656827Z"
    }
   },
   "outputs": [],
   "source": [
    "# from gensim.models import CoherenceModel\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.686004Z",
     "start_time": "2020-10-05T12:01:25.671825Z"
    }
   },
   "outputs": [],
   "source": [
    "# # supporting function\n",
    "# def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "#     lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "#                                            id2word=dictionary,\n",
    "#                                            num_topics=k, \n",
    "#                                            random_state=100,\n",
    "#                                            chunksize=100,\n",
    "#                                            passes=10,\n",
    "#                                            alpha=a,\n",
    "#                                            eta=b)\n",
    "    \n",
    "#     coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "#     return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.702005Z",
     "start_time": "2020-10-05T12:01:25.686827Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tqdm\n",
    "\n",
    "# grid = {}\n",
    "# grid['Validation_Set'] = {}\n",
    "\n",
    "# # Topics range\n",
    "# min_topics = 2\n",
    "# max_topics = 11\n",
    "# step_size = 1\n",
    "# topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# # Alpha parameter\n",
    "# alpha = list(np.arange(0.01, 1, 0.3))\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# # Beta parameter\n",
    "# beta = list(np.arange(0.01, 1, 0.3))\n",
    "# beta.append('symmetric')\n",
    "\n",
    "# # Validation sets\n",
    "# num_of_docs = len(corpus)\n",
    "# corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "#                # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "#                # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "#                corpus]\n",
    "\n",
    "# corpus_title = ['100% Corpus']\n",
    "\n",
    "# model_results = {'Validation_Set': [],\n",
    "#                  'Topics': [],\n",
    "#                  'Alpha': [],\n",
    "#                  'Beta': [],\n",
    "#                  'Coherence': []\n",
    "#                 }\n",
    "\n",
    "# # Can take a long time to run\n",
    "# if 1 == 1:\n",
    "#     pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "#     # iterate through validation corpuses\n",
    "#     for i in range(len(corpus_sets)):\n",
    "#         # iterate through number of topics\n",
    "#         for k in topics_range:\n",
    "#             # iterate through alpha values\n",
    "#             for a in alpha:\n",
    "#                 # iterare through beta values\n",
    "#                 for b in beta:\n",
    "#                     # get the coherence score for the given parameters\n",
    "#                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "#                                                   k=k, a=a, b=b)\n",
    "#                     # Save the model results\n",
    "#                     model_results['Validation_Set'].append(corpus_title[i])\n",
    "#                     model_results['Topics'].append(k)\n",
    "#                     model_results['Alpha'].append(a)\n",
    "#                     model_results['Beta'].append(b)\n",
    "#                     model_results['Coherence'].append(cv)\n",
    "                    \n",
    "#                     pbar.update(1)\n",
    "#     pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:25.717162Z",
     "start_time": "2020-10-05T12:01:25.703828Z"
    }
   },
   "outputs": [],
   "source": [
    "# df =pd.read_csv('./lda_tuning_results.csv',header = 0)\n",
    "# df[df.Coherence == df.Coherence.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:28.254051Z",
     "start_time": "2020-10-05T12:01:25.750854Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)\n",
    "lda_model.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:28.269890Z",
     "start_time": "2020-10-05T12:01:28.255853Z"
    }
   },
   "outputs": [],
   "source": [
    "#lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics = 8, id2word=id2word, passes=10,alpha=0.01,eta=0.9,random_state=100,chunksize=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:28.285044Z",
     "start_time": "2020-10-05T12:01:28.271854Z"
    }
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:28.300887Z",
     "start_time": "2020-10-05T12:01:28.286854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.007*\"covid\" + 0.007*\"test\" + 0.006*\"think\" + 0.006*\"care\" + 0.006*\"get\" + '\n",
      "  '0.006*\"much\" + 0.005*\"health\" + 0.005*\"many\" + 0.004*\"go\" + 0.004*\"day\"'),\n",
      " (1,\n",
      "  '0.008*\"well\" + 0.007*\"covid\" + 0.005*\"care\" + 0.005*\"health\" + 0.004*\"see\" '\n",
      "  '+ 0.004*\"one\" + 0.004*\"strategy\" + 0.004*\"video_tutorial\" + '\n",
      "  '0.004*\"discover\" + 0.004*\"rehabilitate\"'),\n",
      " (2,\n",
      "  '0.012*\"care\" + 0.005*\"see\" + 0.005*\"back\" + 0.005*\"may\" + 0.004*\"health\" + '\n",
      "  '0.004*\"make\" + 0.004*\"leave\" + 0.004*\"give\" + 0.004*\"think\" + 0.003*\"next\"'),\n",
      " (3,\n",
      "  '0.006*\"test\" + 0.005*\"people\" + 0.005*\"covid\" + 0.004*\"want\" + 0.004*\"sad\" '\n",
      "  '+ 0.004*\"make\" + 0.003*\"pandemic\" + 0.003*\"week\" + 0.003*\"feel\" + '\n",
      "  '0.003*\"look\"'),\n",
      " (4,\n",
      "  '0.008*\"health\" + 0.008*\"consider\" + 0.007*\"care\" + 0.006*\"recuperate\" + '\n",
      "  '0.005*\"family\" + 0.005*\"forego\" + 0.004*\"look\" + 0.004*\"covid\" + '\n",
      "  '0.003*\"case\" + 0.003*\"get\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# title = 'Topics of Fear dataset 03-09 Aug'\n",
    "#print(color.BOLD + title + color.END)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:34.392879Z",
     "start_time": "2020-10-05T12:01:34.375888Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "# cloud = WordCloud(stopwords=stop_words,\n",
    "#                   background_color='white',\n",
    "#                   width=2500,\n",
    "#                   height=1800,\n",
    "#                   max_words=10,\n",
    "#                   colormap='tab10',\n",
    "#                   color_func=lambda *args, **kwargs: cols[i],\n",
    "#                   prefer_horizontal=1.0)\n",
    "\n",
    "# topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(10,20), sharex=True, sharey=True)\n",
    "\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#     fig.add_subplot(ax)\n",
    "#     topic_words = dict(topics[i][1])\n",
    "#     cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "#     plt.gca().imshow(cloud)\n",
    "#     plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "#     plt.gca().axis('off')\n",
    "\n",
    "\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)\n",
    "# plt.axis('off')\n",
    "# #plt.title(title)\n",
    "# plt.margins(x=0, y=0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:34.781546Z",
     "start_time": "2020-10-05T12:01:34.761547Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepar_data(text):\n",
    "    text = cleanning(text)\n",
    "    text = gensim.utils.simple_preprocess(str(text), deacc=True) # tokenising\n",
    "    data_words_nostops = remove_stopwords(text) # removing stp words\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) # lametising\n",
    "    data = [x[0] for x in data_lemmatized if x != []]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:39.254089Z",
     "start_time": "2020-10-05T12:01:34.952764Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d6498cea2a412d85736475601ed5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=191.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-193-d2758a68b499>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['tokenised'][i] = prepar_data(dataset['Tweet_clnd'][i])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset['tokenised'] = 0\n",
    "for i in trange(dataset.shape[0]):\n",
    "    tweet = dataset['Tweet_clnd'][i]\n",
    "    dataset['tokenised'][i] = prepar_data(dataset['Tweet_clnd'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:39.347088Z",
     "start_time": "2020-10-05T12:01:39.256090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3ed842d0a945db98920bd453d1a6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=191.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "topics =[]\n",
    "for i in trange(dataset.shape[0]):\n",
    "    toknsd_txt = dataset['tokenised'][i]\n",
    "    new_doc_bow = id2word.doc2bow(toknsd_txt)\n",
    "    topic = lda_model.get_document_topics(new_doc_bow)\n",
    "    topics.append(topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:40.250273Z",
     "start_time": "2020-10-05T12:01:40.232276Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:40.877029Z",
     "start_time": "2020-10-05T12:01:40.872023Z"
    }
   },
   "outputs": [],
   "source": [
    "# tpl1 = dataset['topics'][0]\n",
    "# tpl1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:41.329727Z",
     "start_time": "2020-10-05T12:01:41.319733Z"
    }
   },
   "outputs": [],
   "source": [
    "# tpl2 = dataset['topics'][1664]\n",
    "# tpl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:41.813628Z",
     "start_time": "2020-10-05T12:01:41.800630Z"
    }
   },
   "outputs": [],
   "source": [
    "# topics_s=[]\n",
    "# scores = []\n",
    "\n",
    "# for i in range(len(tpl2)):\n",
    "#     topics_s.append(tpl2[i][0])\n",
    "#     scores.append(tpl2[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:42.062818Z",
     "start_time": "2020-10-05T12:01:42.057818Z"
    }
   },
   "outputs": [],
   "source": [
    "# idx = scores.index(max(scores))\n",
    "# topics_s[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:42.263322Z",
     "start_time": "2020-10-05T12:01:42.248325Z"
    }
   },
   "outputs": [],
   "source": [
    "# def max_idx(lst):\n",
    "#     return lst.index(max(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:42.576156Z",
     "start_time": "2020-10-05T12:01:42.521193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf0b36bfa94495893103ba103e619ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=191.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-201-8fbb36ef4e20>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['topic'][i]=topics[idx]\n"
     ]
    }
   ],
   "source": [
    "dataset['topic'] = 0\n",
    "\n",
    "\n",
    "for i in trange(dataset.shape[0]):\n",
    "    tpls = dataset['topics'][i]\n",
    "    topics =[]\n",
    "    scores =[]\n",
    "    for j in range(len(tpls)):\n",
    "        topics.append(tpls[j][0])\n",
    "        scores.append(tpls[j][1])\n",
    "        idx = scores.index(max(scores))\n",
    "        dataset['topic'][i]=topics[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:44.121507Z",
     "start_time": "2020-10-05T12:01:44.104508Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['Tweet_clnd','tokenised','topics'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T12:01:50.287090Z",
     "start_time": "2020-10-05T12:01:50.215028Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.to_excel('./output/anger_topics_28sep-04oct.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T11:35:24.388464Z",
     "start_time": "2020-08-26T11:35:24.385505Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweet = dataset['Tweet'][4]\n",
    "# tweet = cleanning(tweet) # cleaning \n",
    "# tweet = gensim.utils.simple_preprocess(str(sentence), deacc=True) # tokenising\n",
    "# data_words_nostops = remove_stopwords(tweet) # removing stp words\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops) # making bigrams\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) # lametising\n",
    "# data_lemmatized = [x[0] for x in data_lemmatized if x != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T11:35:28.580734Z",
     "start_time": "2020-08-26T11:35:28.577709Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_doc_bow = id2word.doc2bow(data_lemmatized)\n",
    "# pprint(lda_model.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T14:12:12.657326Z",
     "start_time": "2020-08-25T14:12:12.653301Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_words_nostops = remove_stopwords(tweet)\n",
    "\n",
    "# # Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# data_lemmatized = [x[0] for x in data_lemmatized if x != []]\n",
    "# data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:18:39.609335Z",
     "start_time": "2020-08-25T12:18:39.605832Z"
    }
   },
   "outputs": [],
   "source": [
    "new_doc_bow = id2word.doc2bow(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:18:40.090221Z",
     "start_time": "2020-08-25T12:18:40.086199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.9959005)]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:13:57.565414Z",
     "start_time": "2020-08-25T12:13:57.559430Z"
    }
   },
   "outputs": [],
   "source": [
    "topics =lda_model.get_document_topics(new_doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T13:47:19.022082Z",
     "start_time": "2020-08-25T13:47:18.864412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.6393454), (4, 0.35639077)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "170px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
